## Amazon Bedrock Serverless Prompt Chaining Examples

This repository provides examples of using [AWS Step Functions](https://aws.amazon.com/step-functions/)
and [Amazon Bedrock](https://aws.amazon.com/bedrock/) to build complex, serverless, and highly scalable
generative AI applications with prompt chaining.

[Prompt chaining](https://docs.anthropic.com/claude/docs/prompt-chaining) is a technique for
building complex generative AI applications and accomplishing complex tasks with large language models (LLMs).
With prompt chaining, you construct a set of smaller subtasks as individual prompts. Together, these subtasks
make up your overall complex task that you would like the LLM to complete for your application.
To accomplish the overall task, your application feeds each subtask prompt to the LLM in a pre-defined order or
according to a set of defined rules.

For applications using prompt chaining, Step Functions can orchestrate complex chains of prompts and invoke foundation models in Bedrock.
Beyond simple ordered chains of prompts, Step Functions workflows can contain loops, map jobs, parallel jobs,
conditions, and input/output manipulation. Workflows can also chain together steps that invoke a foundation model in Bedrock,
steps that invoke custom code in Lambda functions, and steps that interact with over 220 AWS services.
Both Bedrock and Step Functions are serverless, so you don't have to manage any infrastructure to deploy and scale up your application.

## Prompt chaining examples

This repository contains several working examples of using the prompt chaining techniques described above,
as part of a demo generative AI application. The [Streamlit-based](https://streamlit.io/) demo application
executes each example's Step Functions state machine and displays the results,
including the content generated by foundation models in Bedrock.
The Step Functions state machines are defined using [AWS CDK](https://aws.amazon.com/cdk/) in Python.

### Write a blog post

This example generates an analysis of a given novel for a literature blog.
This task is broken down into multiple subtasks to first generate individual paragraphs
focused on specific areas of analysis, then a final subtask to pull together the generated
content into a single blog post. The workflow is a simple, sequential chain of prompts.
The previous prompts and LLM responses are carried forward as context and included in
the next prompt as context for the next step in the chain.

![](/webapp/pages/workflow_images/blog_post.png)

CDK code: [stacks/blog_post_stack.py](stacks/blog_post_stack.py)

### Plan a trip

![](/webapp/pages/workflow_images/trip_planner.png)

CDK code: [stacks/trip_planner_stack.py](stacks/trip_planner_stack.py)

### Write a story

![](/webapp/pages/workflow_images/story_writer.png)

CDK code: [stacks/story_writer_stack.py](stacks/story_writer_stack.py)

### Pitch a movie idea

![](/webapp/pages/workflow_images/movie_pitch.png)

CDK code: [stacks/movie_pitch_stack.py](stacks/movie_pitch_stack.py)

### Plan a meal

![](/webapp/pages/workflow_images/meal_planner.png)

CDK code: [stacks/meal_planner_stack.py](stacks/meal_planner_stack.py)

## Deploy

See the [development guide](DEVELOP.md) for instructions on how to deploy the demo application.

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the LICENSE file.
